{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4_Question1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TbWv6YcIQ8lx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S79eSWVvRJro",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8T6qevWPRV8N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import librosa\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "He6Ru8KlYyi8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading train and test data"
      ]
    },
    {
      "metadata": {
        "id": "5_yVowECSVNS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getData(pickle_path):\n",
        "  speech_list = []\n",
        "  mag_speech_list = []\n",
        "  \n",
        "  if os.path.exists(pickle_path):\n",
        "    speech_pickle = pickle.load(open(pickle_path, 'rb'))\n",
        "    print('Pickle already exists! Extracting the data from the pickle')\n",
        "    \n",
        "    for speech in speech_pickle:\n",
        "      S = librosa.stft(speech, n_fft=1024, hop_length=512)\n",
        "      mag_S = np.abs(S)\n",
        "      \n",
        "      speech_list.append(np.transpose(S))\n",
        "      mag_speech_list.append(np.transpose(mag_S))\n",
        "      \n",
        "    return speech_list , mag_speech_list\n",
        "  \n",
        "  else:\n",
        "    print('Pickle doesnt exist! Loading the data from input files')\n",
        "    return speech_list , mag_speech_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wAwBiEPYUkkP",
        "colab_type": "code",
        "outputId": "af51baf0-5b23-45fc-a9f3-f0b2bc0b3aba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "directory_path = '/content/gdrive/My Drive/Colab Notebooks/'\n",
        "train_speech_pickle = directory_path + 'hw4_trs.pkl'\n",
        "\n",
        "train_speech , mag_train_speech  = getData(train_speech_pickle)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pickle already exists! Extracting the data from the pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "11jP3AV2ZwPy",
        "colab_type": "code",
        "outputId": "c953093b-242b-4d88-b0e9-4f138b80f3d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "directory_path = '/content/gdrive/My Drive/Colab Notebooks/'\n",
        "test_speech_pickle = directory_path + 'hw4_tes.pkl'\n",
        "\n",
        "test_speech , mag_test_speech = getData(test_speech_pickle)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pickle already exists! Extracting the data from the pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XRG-kENMb6JY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creating pairs for Saimese Network\n",
        "\n",
        "**Positive pairs :**\n",
        "\n",
        "Speech samples from the same speakers are paired together. Since there are a maximum of 10 speech utterances of the same speaker, we can create a maximum of 45 pairs using $10 \\choose 2$ formula\n",
        "\n",
        "**Negative pairs :**\n",
        "\n",
        "Speech samples from different speakers are paired together"
      ]
    },
    {
      "metadata": {
        "id": "lBDKJljglUQF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_positive_pairs(L , start , end):\n",
        "  pairs = []\n",
        "  for i in range(start , end):\n",
        "    for j in range(i+1 , end):\n",
        "      pairs.append([i , j])\n",
        "  \n",
        "  if L == 45:\n",
        "    return pairs\n",
        "  else:    \n",
        "    L_pairs = random.choices(pairs , k = L)\n",
        "    return L_pairs\n",
        "\n",
        "#Creating a list of speakers from start to end index\n",
        "#If index of a non-speaker is same a the speaker, then we randomly generate an index \n",
        "#till we get an index not belonging to the list of speaker's index list\n",
        "def get_negative_pairs(L , start , end , max_iter):\n",
        "  pairs = []\n",
        "  speakers = list(range(start , end))\n",
        "  for i in range(0 , L):    \n",
        "    speaker = random.randrange(start , end)\n",
        "    non_speaker = random.randrange(0 , max_iter)\n",
        "    \n",
        "    while non_speaker in speakers:\n",
        "      non_speaker = random.randrange(0 , max_iter)\n",
        "      \n",
        "    pairs.append([speaker , non_speaker])\n",
        "  \n",
        "  return pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xNHWxZ8aWQJF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating a list of all positive and negative pairs together using speech samples\n",
        "def make_pairs(x , positive_pairs , negative_pairs):\n",
        "  \n",
        "  ########Positive########\n",
        "  left_positive = x[positive_pairs[0][0]]\n",
        "  right_positive = x[positive_pairs[0][1]]\n",
        "  \n",
        "  x_dim = np.shape(left_positive)[0]\n",
        "  y_dim = np.shape(right_positive)[1]\n",
        "  \n",
        "  left_positive = left_positive.reshape((-1 ,  x_dim , y_dim))\n",
        "  right_positive = right_positive.reshape((-1 ,  x_dim , y_dim))\n",
        "\n",
        "  for j in range(1 , len(positive_pairs)):\n",
        "    lp1 = x[positive_pairs[j][0]].reshape((-1 , x_dim , y_dim))\n",
        "    rp2 = x[positive_pairs[j][1]].reshape((-1 , x_dim , y_dim))\n",
        "    \n",
        "    left_positive = np.vstack((left_positive , lp1))\n",
        "    right_positive = np.vstack((right_positive , rp2))\n",
        "  \n",
        "  ########Negative########\n",
        "  left_negative = x[negative_pairs[0][0]]\n",
        "  right_negative = x[negative_pairs[0][1]]\n",
        "\n",
        "  left_negative = left_negative.reshape((-1 , x_dim , y_dim))\n",
        "  right_negative = right_negative.reshape((-1 , x_dim , y_dim))\n",
        "\n",
        "  for j in range(1 , len(negative_pairs)):\n",
        "    ln1 = x[negative_pairs[j][0]].reshape((-1 , x_dim , y_dim))\n",
        "    rn2 = x[negative_pairs[j][1]].reshape((-1 , x_dim , y_dim))\n",
        "    \n",
        "    left_negative = np.vstack((left_negative , ln1))\n",
        "    right_negative = np.vstack((right_negative , rn2))\n",
        "\n",
        "  all_left = np.vstack((left_positive , left_negative))\n",
        "  all_right = np.vstack((right_positive , right_negative))\n",
        "  \n",
        "  return all_left , all_right"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8VfQ7jsugs6X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Training"
      ]
    },
    {
      "metadata": {
        "id": "W8AT_K5wgxc7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating a model"
      ]
    },
    {
      "metadata": {
        "id": "OzcSc-NsDTWe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getModel(x , gru_hidden_units , time_attr , reuse = False):\n",
        "  cells = []\n",
        "  for i in range(len(gru_hidden_units)):\n",
        "    gru_cell = tf.nn.rnn_cell.GRUCell(num_units= gru_hidden_units[i] , kernel_initializer = tf.contrib.layers.xavier_initializer(), reuse=reuse , name='GRU_cell_'+str(i))\n",
        "    cells.append(gru_cell)\n",
        "  multi_gru_cells = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
        "\n",
        "  gru_output , state = tf.nn.dynamic_rnn(multi_gru_cells , x , dtype=tf.float32 , swap_memory = True)\n",
        "  batch_gru = tf.contrib.layers.batch_norm(gru_output, is_training=True, updates_collections=None)\n",
        "  \n",
        "  output = tf.layers.dense(inputs=batch_gru , units=30, activation = tf.nn.tanh ,\n",
        "                           kernel_initializer = tf.contrib.layers.variance_scaling_initializer(),\n",
        "                           bias_initializer = tf.zeros_initializer(), \n",
        "                           reuse = reuse, name='op')\n",
        "  \n",
        "  dropout = tf.nn.dropout(output , keep_prob = 0.9 , name = 'drop')\n",
        "  \n",
        "  output1 = tf.layers.dense(inputs=output , units=10, activation = tf.nn.tanh ,\n",
        "                           kernel_initializer = tf.contrib.layers.variance_scaling_initializer(),\n",
        "                           bias_initializer = tf.zeros_initializer(), \n",
        "                           reuse = reuse, name='op1')\n",
        "  \n",
        "  reshapedOutput = tf.reshape(output1,shape = [-1,time_attr*10])\n",
        "  \n",
        "  return reshapedOutput"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qanIHXidko10",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining model parameters"
      ]
    },
    {
      "metadata": {
        "id": "YgcMrkEK-y6H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "left_input = tf.placeholder(tf.float32, [None, None , 513])\n",
        "right_input = tf.placeholder(tf.float32, [None, None , 513])\n",
        "label = tf.placeholder(tf.float32, [None])\n",
        "time_attr = tf.placeholder(tf.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cr-raArKFdab",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gru_hidden_units = [256 , 128]\n",
        "learning_rate = 0.001\n",
        "num_epochs = 200\n",
        "num_utterances = 10\n",
        "L = 45"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_89JLQWTh_zA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating a Siamese model \n",
        "\n",
        "The model takes two spectograms as input (left and right). Weights of the model are shared across both the sides"
      ]
    },
    {
      "metadata": {
        "id": "ozLIeK2B_tA0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "left_output = getModel(left_input , gru_hidden_units, time_attr , reuse =  False)\n",
        "right_output = getModel(right_input , gru_hidden_units , time_attr , reuse = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7l9nycIRpJLj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "Loss function  - $-y(x_{i},x_{j})log \\sigma(z_{i}^T z_{j}) - (1 - y(x_{i}, x_{j}))log(1 - \\sigma(z_{i}^T z_{j}))$\n",
        "\n",
        "Siamese network will try to predict 1 for positive pairs and 0 for negative pairs"
      ]
    },
    {
      "metadata": {
        "id": "Yz5LPZ2TAMhM",
        "colab_type": "code",
        "outputId": "b93bc63f-5414-499f-84d0-790d37147281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1867
        }
      },
      "cell_type": "code",
      "source": [
        "#Defining the loss function along with its optimizer\n",
        "label_pred = tf.reduce_sum(tf.multiply(left_output, right_output),axis=1 , name='dotprod')\n",
        "label_pred_sig = tf.nn.sigmoid(label_pred)\n",
        "# loss = -(label) * tf.log(label_pred+ (1e-6)) - (1 - label) * tf.log(1 - label_pred + (1e-6))\n",
        "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels= label , logits= label_pred)\n",
        "loss_calculate = tf.reduce_sum(loss)\n",
        "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_calculate)\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "loss_list = []\n",
        "true_label = []\n",
        "count = 0\n",
        "flag = True\n",
        "\n",
        "while flag:\n",
        "    #Mini batching\n",
        "    for i in range(0 , len(train_speech), num_utterances):\n",
        "      start = i\n",
        "      end = i + num_utterances\n",
        "      positive_pairs = get_positive_pairs(L , start , end)\n",
        "      negative_pairs = get_negative_pairs(L , start , end , len(train_speech))\n",
        "\n",
        "      left , right = make_pairs(mag_train_speech , positive_pairs , negative_pairs)\n",
        "      \n",
        "      if count == 0:    \n",
        "        ones = np.ones(L)\n",
        "        zeroes = np.zeros(L)\n",
        "\n",
        "        true_label.extend(ones)\n",
        "        true_label.extend(zeroes)\n",
        "\n",
        "      if i == 0:\n",
        "        all_left = left\n",
        "        all_right = right\n",
        "      else:\n",
        "        all_left = np.vstack((all_left , left))\n",
        "        all_right = np.vstack((all_right , right))\n",
        "    \n",
        "    train_time_attr = all_left.shape[1]\n",
        "    feed_dict = {left_input: all_left, right_input: all_right, time_attr:train_time_attr, label: true_label}\n",
        "    train_step.run(feed_dict=feed_dict)\n",
        "\n",
        "    \n",
        "    if count%2 == 0:\n",
        "        loss_calc = loss_calculate.eval(feed_dict=feed_dict)\n",
        "        loss_list.append((loss_calc))\n",
        "        print(\"Epoch %d, loss %g\"%(count, loss_calc))\n",
        "    \n",
        "    #Once all the epochs are completed, training is stopped\n",
        "    if count >= num_epochs:\n",
        "        flag = False  \n",
        "        \n",
        "    count+=1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 0, loss 175577\n",
            "Epoch 2, loss 89604.3\n",
            "Epoch 4, loss 62977.1\n",
            "Epoch 6, loss 52203.6\n",
            "Epoch 8, loss 38445.1\n",
            "Epoch 10, loss 34441.9\n",
            "Epoch 12, loss 29935.4\n",
            "Epoch 14, loss 25550.6\n",
            "Epoch 16, loss 20449.2\n",
            "Epoch 18, loss 18564.6\n",
            "Epoch 20, loss 15437.2\n",
            "Epoch 22, loss 14139.6\n",
            "Epoch 24, loss 11703.6\n",
            "Epoch 26, loss 11090.5\n",
            "Epoch 28, loss 9425.74\n",
            "Epoch 30, loss 8693\n",
            "Epoch 32, loss 7324.52\n",
            "Epoch 34, loss 6896.5\n",
            "Epoch 36, loss 6187.86\n",
            "Epoch 38, loss 5515.79\n",
            "Epoch 40, loss 5016.19\n",
            "Epoch 42, loss 4925.97\n",
            "Epoch 44, loss 4526.18\n",
            "Epoch 46, loss 4313.6\n",
            "Epoch 48, loss 4194.38\n",
            "Epoch 50, loss 3765.99\n",
            "Epoch 52, loss 3606.02\n",
            "Epoch 54, loss 3433.05\n",
            "Epoch 56, loss 3205.41\n",
            "Epoch 58, loss 3118.43\n",
            "Epoch 60, loss 3069.79\n",
            "Epoch 62, loss 2897.85\n",
            "Epoch 64, loss 2874.73\n",
            "Epoch 66, loss 2752.17\n",
            "Epoch 68, loss 2717.27\n",
            "Epoch 70, loss 2685.33\n",
            "Epoch 72, loss 2596.65\n",
            "Epoch 74, loss 2602.96\n",
            "Epoch 76, loss 2488.8\n",
            "Epoch 78, loss 2403.98\n",
            "Epoch 80, loss 2376.8\n",
            "Epoch 82, loss 2336.95\n",
            "Epoch 84, loss 2334.72\n",
            "Epoch 86, loss 2231.37\n",
            "Epoch 88, loss 2138.1\n",
            "Epoch 90, loss 2102.34\n",
            "Epoch 92, loss 2094.02\n",
            "Epoch 94, loss 2054.23\n",
            "Epoch 96, loss 2086.49\n",
            "Epoch 98, loss 1975.23\n",
            "Epoch 100, loss 1952.85\n",
            "Epoch 102, loss 1942.38\n",
            "Epoch 104, loss 1884.46\n",
            "Epoch 106, loss 1823.1\n",
            "Epoch 108, loss 1829.13\n",
            "Epoch 110, loss 1863.76\n",
            "Epoch 112, loss 1731.58\n",
            "Epoch 114, loss 1733.51\n",
            "Epoch 116, loss 1684.28\n",
            "Epoch 118, loss 1631.05\n",
            "Epoch 120, loss 1662.03\n",
            "Epoch 122, loss 1649.48\n",
            "Epoch 124, loss 1614.18\n",
            "Epoch 126, loss 1624.04\n",
            "Epoch 128, loss 1583.26\n",
            "Epoch 130, loss 1485.46\n",
            "Epoch 132, loss 1526.87\n",
            "Epoch 134, loss 1476.65\n",
            "Epoch 136, loss 1477.47\n",
            "Epoch 138, loss 1493.29\n",
            "Epoch 140, loss 1454.36\n",
            "Epoch 142, loss 1470.43\n",
            "Epoch 144, loss 1373.05\n",
            "Epoch 146, loss 1359.87\n",
            "Epoch 148, loss 1312.62\n",
            "Epoch 150, loss 1339.39\n",
            "Epoch 152, loss 1317.51\n",
            "Epoch 154, loss 1260.72\n",
            "Epoch 156, loss 1234.63\n",
            "Epoch 158, loss 1283.28\n",
            "Epoch 160, loss 1286.07\n",
            "Epoch 162, loss 1237.37\n",
            "Epoch 164, loss 1214.78\n",
            "Epoch 166, loss 1136.87\n",
            "Epoch 168, loss 1152.9\n",
            "Epoch 170, loss 1112.73\n",
            "Epoch 172, loss 1140.6\n",
            "Epoch 174, loss 1086\n",
            "Epoch 176, loss 1084.71\n",
            "Epoch 178, loss 1053.18\n",
            "Epoch 180, loss 1030.68\n",
            "Epoch 182, loss 998.584\n",
            "Epoch 184, loss 969.884\n",
            "Epoch 186, loss 989.634\n",
            "Epoch 188, loss 1064.98\n",
            "Epoch 190, loss 1062.87\n",
            "Epoch 192, loss 993.627\n",
            "Epoch 194, loss 948.158\n",
            "Epoch 196, loss 940.168\n",
            "Epoch 198, loss 911.075\n",
            "Epoch 200, loss 928.933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zPJkNbmroObC",
        "colab_type": "code",
        "outputId": "52257083-b657-4a21-f012-8a053d450310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(loss_list)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff9180a35c0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmEPDzMwa9v8BnVQHH4X1cGEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "6AkAZuQnn7ZX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "\n",
        "## Creating positive and negative pairs for testing"
      ]
    },
    {
      "metadata": {
        "id": "sU79LCssjcQU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "true_label = []\n",
        "for i in range(0 , len(test_speech), num_utterances):\n",
        "  start = i\n",
        "  end = i + num_utterances\n",
        "  test_positive_pairs = get_positive_pairs(L , start , end)\n",
        "  test_negative_pairs = get_negative_pairs(L , start , end , len(test_speech))\n",
        "\n",
        "  test_left , test_right = make_pairs(mag_test_speech , test_positive_pairs , test_negative_pairs)\n",
        "  \n",
        "  \n",
        "  ones = np.ones(L)\n",
        "  zeroes = np.zeros(L)\n",
        "\n",
        "  true_label.extend(ones)\n",
        "  true_label.extend(zeroes)\n",
        "\n",
        "  if i == 0:\n",
        "    test_all_left = test_left\n",
        "    test_all_right = test_right\n",
        "  else:\n",
        "    test_all_left = np.vstack((test_all_left , test_left))\n",
        "    test_all_right = np.vstack((test_all_right , test_right))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kg7wzcERveSc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Predicting output for test data"
      ]
    },
    {
      "metadata": {
        "id": "H8iZqUIHR3Lw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_time_attr = test_all_left.shape[1]\n",
        "feed_dict = {left_input: test_all_left, right_input: test_all_right, time_attr:test_time_attr}\n",
        "\n",
        "predicted_output = label_pred_sig.eval(feed_dict=feed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "puPzVPMZvOU_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Binarising predicted output"
      ]
    },
    {
      "metadata": {
        "id": "mm5mftavSEiR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(predicted_output)):\n",
        "  if predicted_output[i] >= 0.5:\n",
        "    predicted_output[i] = 1\n",
        "  else:\n",
        "    predicted_output[i] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HP3GnjGcvW6A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculating the accuracy of test data"
      ]
    },
    {
      "metadata": {
        "id": "psSiCpLwSpQn",
        "colab_type": "code",
        "outputId": "90627f1a-14b7-4e12-927a-822c85f2e137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "accuracy = sum(predicted_output == true_label)\n",
        "print('Accuracy for test data : ' + str(accuracy/len(predicted_output)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for test data : 0.6833333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}